{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code builds the model. Make sure that you upload the files and then you will have a pickl file with labels and another file with model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**\n",
    "\n",
    "Note: Imutils package is mostly not supported on Kaggle, therefore to install it\n",
    "* Turn on internet access for the current notebook\n",
    "* Run commant '!pip install imutils' in the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import imutils\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "pathCAPTCHA = \"/kaggle/input\"\n",
    "pathModel = \"/kaggle/output/working\"\n",
    "pathLetters = \"/kaggle/output/working\"\n",
    "modelName = \"PROVEYOUAREHUMAN.pkl\"\n",
    "lableName = \"labels.dat\"\n",
    "\n",
    "# Sample Image (Relatively cleaner one!)\n",
    "sampleImage = cv2.imread(pathCAPTCHA+\"/S9YE.png\",0)\n",
    "plt.imshow(sampleImage,'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resize Image**\n",
    "Resizing is essential as Keras uses a specific dimension of image only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function: resizeImage - Resizes the image into given width and height\n",
    "used to reload the digits cut from each CAPTCHA\n",
    ":param image: image to resize\n",
    ":param width: desired width in pixels\n",
    ":param height: desired height in pixels\n",
    ":return: the resized image\n",
    "\"\"\"\n",
    "def resizeImage(image, width, height):\n",
    "\n",
    "    # grab the dimensions of the image, then initialize\n",
    "    # the padding values\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if the width is greater than the height then resize along\n",
    "    # the width\n",
    "    if w > h:\n",
    "        image = imutils.resize(image, width=width)\n",
    "\n",
    "    # otherwise, the height is greater than the width so resize\n",
    "    # along the height\n",
    "    else:\n",
    "        image = imutils.resize(image, height=height)\n",
    "\n",
    "    # determine the padding values for the width and height to\n",
    "    # obtain the target dimensions\n",
    "    padW = int((width - image.shape[1]) / 2.0)\n",
    "    padH = int((height - image.shape[0]) / 2.0)\n",
    "\n",
    "    # pad the image then apply one more resizing to handle any\n",
    "    # rounding issues\n",
    "    image = cv2.copyMakeBorder(image, padH, padH, padW, padW,\n",
    "        cv2.BORDER_REPLICATE)\n",
    "    image = cv2.resize(image, (width, height))\n",
    "\n",
    "    # return the pre-processed image\n",
    "    return image\n",
    "def alter(text):\n",
    "    #for i in text:\n",
    "    #    if i == '1':\n",
    "    #        i = 'I'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object that iterates and consists all the CAPTCHAs\n",
    "imageDataStore = glob.glob(os.path.join(pathCAPTCHA, \"*\"))\n",
    "length = len(imageDataStore)\n",
    "# Count to store each of the different letter text in the directory\n",
    "# i.e. for a dataset of 30K images, 120K different letters are stored\n",
    "counts = {}\n",
    "\n",
    "\n",
    "for (i, imds) in enumerate(imageDataStore):\n",
    "    \n",
    "    print(\"[PREPROCESSING] image {} out of {}\".format(i + 1, length))\n",
    "\n",
    "    # Since the filename consists the label, we need to extract that from the filepath\n",
    "    filename = os.path.basename(imds)\n",
    "    text = os.path.splitext(filename)[0]\n",
    "    text = alter(text)\n",
    "\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(imds)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Add some extra padding around the image\n",
    "    gray = cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # threshold the image (convert it to pure black and white)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # find the contours (continuous blobs of pixels) the image\n",
    "    contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Hack for compatibility with different OpenCV versions\n",
    "    contours = contours[0] #if imutils.is_cv2() else contours[1]\n",
    "\n",
    "    letter_image_regions = []\n",
    "\n",
    "    # Now we can loop through each of the four contours and extract the letter\n",
    "    # inside of each one\n",
    "    for contour in contours:\n",
    "        # Get the rectangle that contains the contour\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        # Compare the width and height of the contour to detect letters that\n",
    "        # are conjoined into one chunk\n",
    "        if w / h > 1.25:\n",
    "            # This contour is too wide to be a single letter!\n",
    "            # Split it in half into two letter regions!\n",
    "            half_width = int(w / 2)\n",
    "            letter_image_regions.append((x, y, half_width, h))\n",
    "            letter_image_regions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            # This is a normal letter by itself\n",
    "            letter_image_regions.append((x, y, w, h))\n",
    "\n",
    "    # If we found more or less than 4 letters in the captcha, our letter extraction\n",
    "    # didn't work correcly. Skip the image instead of saving bad training data!\n",
    "    if len(letter_image_regions) != 4:\n",
    "        continue\n",
    "\n",
    "    # Sort the detected letter images based on the x coordinate to make sure\n",
    "    # we are processing them from left-to-right so we match the right image\n",
    "    # with the right letter\n",
    "    letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])\n",
    "\n",
    "    # Save out each letter as a single image\n",
    "    for letter_bounding_box, letter_text in zip(letter_image_regions, text):\n",
    "        # Grab the coordinates of the letter in the image\n",
    "        x, y, w, h = letter_bounding_box\n",
    "\n",
    "        # Extract the letter from the original image with a 2-pixel margin around the edge\n",
    "        letter_image = gray[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "\n",
    "        # Get the folder to save the image in model folder, todo - delete it later\n",
    "        save_path = os.path.join(pathModel, letter_text)\n",
    "\n",
    "        # Creating a letter image and saving it\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        count = counts.get(letter_text, 1)\n",
    "        p = os.path.join(save_path, \"{}.png\".format(str(count).zfill(6)))\n",
    "        cv2.imwrite(p, letter_image)\n",
    "        counts[letter_text] = count + 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_file in paths.list_images(pathLetters):\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Resize the letter so it fits in a 20x20 pixel box\n",
    "    image = resizeImage(image, 20, 20)\n",
    "    # Add a third channel dimension to the image escape less channel error\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "    # Grab the name of the letter based on the folder it was in\n",
    "    label = image_file.split(os.path.sep)[-2]\n",
    "\n",
    "    # Add the letter image and it's label to our training data\n",
    "    dataset.append(image)\n",
    "    labels.append(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traing the CNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the raw pixel intensities to the range [0, 1] (this improves training)\n",
    "dataset = np.array(dataset, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the training data into separate train and test sets\n",
    "(X_train, X_validate, Y_train, Y_validate) = train_test_split(dataset, labels, test_size=0.25, random_state=0)\n",
    "\n",
    "# Convert the labels (letters) into one-hot encodings that Keras can work with\n",
    "lb = LabelBinarizer().fit(Y_train)\n",
    "Y_train = lb.transform(Y_train)\n",
    "Y_validate = lb.transform(Y_validate)\n",
    "\n",
    "# Save the mapping from labels to one-hot encodings.\n",
    "# We'll need this later when we use the model to decode what it's predictions mean\n",
    "with open(lableName, \"wb\") as f:\n",
    "    pickle.dump(lb, f)\n",
    "\n",
    "# Build the neural network!\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer with max pooling\n",
    "model.add(Conv2D(20, (5, 5), padding=\"same\", input_shape=(20, 20, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Second convolutional layer with max pooling\n",
    "model.add(Conv2D(50, (5, 5), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Hidden layer with 500 nodes\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(36, activation=\"softmax\"))\n",
    "\n",
    "# Ask Keras to build the TensorFlow model behind the scenes\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, Y_train, validation_data=(X_validate, Y_validate), batch_size=32, epochs=10, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"PROVEYOUAREHUMAN.pkl\"\n",
    "file = open(modelName,'wb')\n",
    "pickle.dump(model,file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
